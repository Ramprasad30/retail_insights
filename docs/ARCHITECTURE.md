# ðŸ—ï¸ Retail Insights Assistant - System Architecture

## Table of Contents

1. [System Overview](#system-overview)
2. [Multi-Agent Architecture](#multi-agent-architecture)
3. [Data Flow](#data-flow)
4. [Component Details](#component-details)
5. [Scalability Architecture (100GB+)](#scalability-architecture)
6. [Cost Analysis](#cost-analysis)
7. [Performance Considerations](#performance-considerations)

## System Overview

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERFACE LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Streamlit Web Application                       â”‚  â”‚
â”‚  â”‚  â€¢ Summary Dashboard  â€¢ Q&A Chat  â€¢ Visualizations       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   APPLICATION LOGIC LAYER                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            Multi-Agent System (LangGraph)                 â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚  â”‚  Query   â”‚â†’ â”‚   Data   â”‚â†’ â”‚Validationâ”‚â†’ â”‚Synthesis â”‚ â”‚  â”‚
â”‚  â”‚  â”‚Resolutionâ”‚  â”‚Extractionâ”‚  â”‚  Agent   â”‚  â”‚  Agent   â”‚ â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              LLM Integration (OpenAI GPT-4)               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ SQL Queries
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      DATA PROCESSING LAYER                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              DuckDB Analytical Database                   â”‚  â”‚
â”‚  â”‚  â€¢ In-memory processing  â€¢ Columnar storage              â”‚  â”‚
â”‚  â”‚  â€¢ SQL query engine      â€¢ Analytical functions          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ Data Loading
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DATA STORAGE LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Amazon    â”‚  â”‚Internationalâ”‚  â”‚ Inventory  â”‚  â”‚   P&L    â”‚ â”‚
â”‚  â”‚   Sales    â”‚  â”‚   Sales     â”‚  â”‚   Data     â”‚  â”‚  Data    â”‚ â”‚
â”‚  â”‚ (128K rows)â”‚  â”‚  (37K rows) â”‚  â”‚  (9K rows) â”‚  â”‚(1K rows) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     CSV Files on Disk                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Multi-Agent Architecture

### Agent Communication Flow

```
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   User Query    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Shared State   â”‚
                      â”‚  (AgentState)   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘       AGENT 1: Query Resolution Agent        â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘ Input:  User natural language query          â•‘
        â•‘ Process:                                      â•‘
        â•‘   1. Parse intent and context                â•‘
        â•‘   2. Identify required tables/columns        â•‘
        â•‘   3. Generate SQL query                      â•‘
        â•‘   4. Determine query type (summary/qa)       â•‘
        â•‘ Output: SQL query string, query metadata     â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Update State   â”‚
                      â”‚  sql_query      â”‚
                      â”‚  query_type     â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘      AGENT 2: Data Extraction Agent          â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘ Input:  SQL query, database connection       â•‘
        â•‘ Process:                                      â•‘
        â•‘   1. Execute SQL via DuckDB                  â•‘
        â•‘   2. Handle errors and edge cases            â•‘
        â•‘   3. Limit results (pagination)              â•‘
        â•‘   4. Format data structure                   â•‘
        â•‘ Output: DataFrame / Dictionary of results    â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Update State   â”‚
                      â”‚  data_result    â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘        AGENT 3: Validation Agent             â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘ Input:  Query results, expected schema       â•‘
        â•‘ Process:                                      â•‘
        â•‘   1. Check for errors                        â•‘
        â•‘   2. Validate data availability              â•‘
        â•‘   3. Verify data structure                   â•‘
        â•‘   4. Quality assurance checks                â•‘
        â•‘ Output: Validation status (PASS/WARN/FAIL)   â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Update State   â”‚
                      â”‚validation_statusâ”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘        AGENT 4: Synthesis Agent              â•‘
        â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
        â•‘ Input:  Validated data, user query, context  â•‘
        â•‘ Process:                                      â•‘
        â•‘   1. Analyze data patterns                   â•‘
        â•‘   2. Generate insights                       â•‘
        â•‘   3. Format response in natural language     â•‘
        â•‘   4. Add business context                    â•‘
        â•‘ Output: Human-readable response with insightsâ•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                               â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  Final Response â”‚
                      â”‚  to User        â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### State Management

```python
class AgentState(TypedDict):
    """Shared state across all agents"""
    messages: List[BaseMessage]      # Conversation history
    user_query: str                  # Original user question
    query_type: str                  # 'summary' or 'qa'
    sql_query: str                   # Generated SQL
    data_result: Any                 # Query results
    validation_status: str           # 'PASSED', 'WARNING', 'FAILED'
    final_response: str              # Final answer
    schema_info: Dict                # Database schema
    summary_stats: Dict              # Pre-computed statistics
    iteration: int                   # Current iteration count
```

### LangGraph Workflow Definition

```python
from langgraph.graph import StateGraph, END

# Create workflow graph
workflow = StateGraph(AgentState)

# Add agent nodes
workflow.add_node("query_resolution", query_resolution_agent)
workflow.add_node("data_extraction", data_extraction_agent)
workflow.add_node("validation", validation_agent)
workflow.add_node("synthesis", synthesis_agent)

# Define linear workflow
workflow.set_entry_point("query_resolution")
workflow.add_edge("query_resolution", "data_extraction")
workflow.add_edge("data_extraction", "validation")
workflow.add_edge("validation", "synthesis")
workflow.add_edge("synthesis", END)

# Compile to executable
graph = workflow.compile()
```

## Data Flow

### Summary Mode Flow

```
User clicks "Generate Summary"
        â”‚
        â”œâ”€> Load all datasets into DuckDB
        â”‚   â”œâ”€> Amazon Sales (128K rows)
        â”‚   â”œâ”€> International Sales (37K rows)
        â”‚   â””â”€> Inventory (9K rows)
        â”‚
        â”œâ”€> Execute pre-defined aggregation queries
        â”‚   â”œâ”€> Total revenue, orders, metrics
        â”‚   â”œâ”€> Top categories analysis
        â”‚   â”œâ”€> Regional performance
        â”‚   â””â”€> Status distribution
        â”‚
        â”œâ”€> Generate visualizations
        â”‚   â”œâ”€> Bar charts (categories, states)
        â”‚   â”œâ”€> Pie charts (status distribution)
        â”‚   â””â”€> Metric cards
        â”‚
        â”œâ”€> Pass to Multi-Agent System
        â”‚   â””â”€> Synthesis Agent generates insights
        â”‚
        â””â”€> Display dashboard + AI insights
```

### Q&A Mode Flow

```
User enters question: "What are top selling categories?"
        â”‚
        â”œâ”€> Query Resolution Agent
        â”‚   â”œâ”€> Analyze intent: "top selling" = revenue/quantity ranking
        â”‚   â”œâ”€> Identify table: amazon_sales
        â”‚   â”œâ”€> Identify columns: Category, Amount
        â”‚   â””â”€> Generate SQL:
        â”‚       SELECT Category, COUNT(*) as orders, 
        â”‚              SUM(Amount) as revenue
        â”‚       FROM amazon_sales
        â”‚       GROUP BY Category
        â”‚       ORDER BY revenue DESC
        â”‚       LIMIT 5
        â”‚
        â”œâ”€> Data Extraction Agent
        â”‚   â”œâ”€> Execute SQL query
        â”‚   â”œâ”€> Retrieve results (5 rows)
        â”‚   â””â”€> Format as dictionary
        â”‚
        â”œâ”€> Validation Agent
        â”‚   â”œâ”€> Check: Query executed successfully âœ“
        â”‚   â”œâ”€> Check: Results returned âœ“
        â”‚   â”œâ”€> Check: Data structure valid âœ“
        â”‚   â””â”€> Status: PASSED
        â”‚
        â”œâ”€> Synthesis Agent
        â”‚   â”œâ”€> Analyze: Top category is "Kurta" with â‚¹15.2M
        â”‚   â”œâ”€> Context: Represents 29% of total revenue
        â”‚   â”œâ”€> Insight: Strong performance in traditional wear
        â”‚   â””â”€> Generate natural language response
        â”‚
        â””â”€> Display formatted answer to user
```

## Component Details

### 1. Data Processor (DuckDB)

**File:** `data_processor.py`

**Responsibilities:**
- Load CSV files into DuckDB tables
- Provide schema information
- Execute SQL queries
- Generate summary statistics
- Handle data search

**Key Methods:**

```python
class RetailDataProcessor:
    def load_data() -> Dict[str, Any]
        # Load all CSV files into DuckDB tables
        
    def execute_query(query: str) -> pd.DataFrame
        # Execute SQL and return results
        
    def get_schema_info() -> Dict[str, List[str]]
        # Return table schemas
        
    def get_summary_statistics() -> Dict[str, Any]
        # Pre-computed aggregate statistics
```

**DuckDB Advantages:**
- In-memory processing (fast)
- Columnar storage (efficient aggregations)
- SQL interface (familiar)
- Pandas integration (seamless)
- No server required (embedded)

### 2. Multi-Agent System (LangGraph)

**File:** `agents.py`

**Architecture Pattern:** Agent-Based Workflow

**Design Principles:**
1. **Single Responsibility**: Each agent has one clear purpose
2. **Sequential Processing**: Linear workflow with state passing
3. **Error Handling**: Each agent can fail gracefully
4. **State Immutability**: Agents update state, don't mutate

**Agent Specializations:**

| Agent | LLM Usage | Data Access | Output |
|-------|-----------|-------------|--------|
| Query Resolution | Yes (SQL generation) | Schema only | SQL query |
| Data Extraction | No | Full database | Raw data |
| Validation | No | Result set | Status |
| Synthesis | Yes (insight generation) | Query results | Natural language |

### 3. Streamlit UI

**File:** `app.py`

**Architecture Pattern:** Component-Based UI

**Components:**
1. **Sidebar**: Configuration and mode selection
2. **Main Panel**: Content display
3. **Chat Interface**: Q&A history
4. **Dashboard**: Summary visualizations

**State Management:**
```python
# Session state for chat history
st.session_state.messages = []

# Cached resources (expensive operations)
@st.cache_resource
def initialize_assistant(api_key):
    return MultiAgentRetailAssistant(api_key)
```

### 4. Configuration Management

**File:** `config.py`

**Centralized Configuration:**
- API keys (environment variables)
- Model parameters
- Data paths
- System limits
- Feature flags

## Scalability Architecture (100GB+)

### Phase 1: Current Architecture (Up to 10GB)

```
Local Machine
â”œâ”€â”€ CSV Files (disk)
â”œâ”€â”€ DuckDB (in-memory)
â”œâ”€â”€ Python Application
â””â”€â”€ Streamlit UI
```

**Limitations:**
- Single machine memory
- No horizontal scaling
- Manual data updates
- Limited concurrency

### Phase 2: Cloud-Ready Architecture (10GB - 100GB)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Application Tier (Docker)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Streamlit â”‚  â”‚  Agents  â”‚  â”‚  Data    â”‚ â”‚
â”‚  â”‚    UI    â”‚  â”‚  System  â”‚  â”‚Processor â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Redis Cache Layer                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DuckDB / Trino Query Engine         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Object Storage (S3/GCS/Azure)          â”‚
â”‚         Parquet Files (partitioned)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Improvements:**
- Containerized deployment
- Result caching
- Parquet format (10x smaller)
- Partitioned storage

### Phase 3: Enterprise Architecture (100GB+)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Load Balancer  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  App Instance  â”‚  â”‚  App Instance  â”‚  â”‚  App Instance  â”‚
â”‚   (Container)  â”‚  â”‚   (Container)  â”‚  â”‚   (Container)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   Redis Cluster (Cache)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Pinecone     â”‚  â”‚   BigQuery     â”‚  â”‚    Spark       â”‚
â”‚ (Vector Store) â”‚  â”‚  (Data Warehouse)â”‚ â”‚  (Processing)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Cloud Storage  â”‚
                    â”‚ (Data Lake)     â”‚
                    â”‚  Delta Lake     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Components:**

#### A. Data Ingestion Pipeline

```
Raw Data Sources
    â†“
Apache Kafka / Cloud Pub/Sub (streaming)
    â†“
Apache Spark / Dask (processing)
    â”œâ”€> Validation & Cleaning
    â”œâ”€> Schema Enforcement
    â”œâ”€> Partitioning Strategy
    â””â”€> Format Conversion (Parquet/Delta)
    â†“
Cloud Storage (S3/GCS/Azure)
    â”œâ”€> Raw Zone (immutable)
    â”œâ”€> Curated Zone (cleaned)
    â””â”€> Aggregated Zone (pre-computed)
    â†“
Data Warehouse (BigQuery/Snowflake)
```

#### B. Query Routing Strategy

```python
def route_query(user_query, estimated_size):
    """Intelligent query routing"""
    
    # 1. Check cache first
    if cached_result := cache.get(hash(user_query)):
        return cached_result
    
    # 2. Check if pre-aggregated data available
    if is_aggregatable(user_query):
        return query_aggregated_table(user_query)
    
    # 3. Use vector search for semantic queries
    if is_semantic_query(user_query):
        context = vector_store.search(user_query)
        return llm_with_context(user_query, context)
    
    # 4. For large scans, use distributed engine
    if estimated_size > THRESHOLD:
        return spark_query(user_query)
    
    # 5. Default to DuckDB for medium queries
    return duckdb_query(user_query)
```

#### C. Vector Store Integration

```
Pre-Processing:
    â”œâ”€> Generate embeddings for:
    â”‚   â”œâ”€> Monthly summaries
    â”‚   â”œâ”€> Category insights
    â”‚   â”œâ”€> Regional reports
    â”‚   â””â”€> Product descriptions
    â†“
Store in Vector DB (Pinecone/Weaviate)
    â†“
Query Time:
    â”œâ”€> Embed user query
    â”œâ”€> Similarity search (top K)
    â”œâ”€> Retrieve relevant context
    â””â”€> Augment LLM prompt
```

#### D. Cost Optimization Strategies

**1. Query Result Caching**
```python
# Cache frequently asked queries
cache_ttl = {
    "summary": 3600,      # 1 hour
    "category_top": 1800, # 30 minutes
    "daily_stats": 300    # 5 minutes
}
```

**2. Model Selection**
```python
# Use appropriate model for complexity
if query_complexity == "simple":
    model = "gpt-3.5-turbo"  # $0.001/1K tokens
elif query_complexity == "medium":
    model = "gpt-4o"         # $0.005/1K tokens
else:
    model = "gpt-4-turbo"    # $0.01/1K tokens
```

**3. Pre-Aggregation**
```sql
-- Maintain materialized views
CREATE MATERIALIZED VIEW daily_category_sales AS
SELECT 
    DATE_TRUNC('day', date) as day,
    category,
    SUM(amount) as revenue,
    COUNT(*) as orders
FROM sales
GROUP BY day, category;
```

## Cost Analysis

### Current Implementation (OpenAI GPT-4)

**Per Query Cost Breakdown:**

| Component | Cost | Notes |
|-----------|------|-------|
| Query Resolution | $0.03 | ~1K input + 500 output tokens |
| Synthesis | $0.05 | ~2K input + 1K output tokens |
| **Total per Q&A** | **$0.08** | Average per question |
| Summary Generation | $0.15 | Larger context |

**Monthly Cost Estimate:**
- 1,000 queries/month: ~$80
- 10,000 queries/month: ~$800
- 100,000 queries/month: ~$8,000

**Optimization Strategies:**
1. Cache common queries (80% hit rate = 80% cost savings)
2. Use GPT-3.5 for simple queries (10x cheaper)
3. Batch similar queries
4. Pre-compute summaries

### Scalable Architecture Costs (100GB data)

**Infrastructure (Cloud - AWS example):**

| Service | Spec | Monthly Cost |
|---------|------|--------------|
| EKS Cluster | 3 x t3.xlarge | $250 |
| BigQuery | 100GB + 1TB queries | $150 |
| S3 Storage | 100GB | $2.30 |
| Redis Cache | r6g.large | $120 |
| Pinecone | 1M vectors | $70 |
| Load Balancer | ALB | $20 |
| **Total Infrastructure** | | **~$612/month** |

**LLM Costs:**
- GPT-4: ~$800/month (10K queries)
- Caching saves: ~$640/month (80% hit rate)
- **Net LLM Cost**: **~$160/month**

**Total Monthly Cost:** ~$772 for 10,000 queries

## Performance Considerations

### Latency Breakdown

**Current System (Small Data):**
```
User Query
â”‚
â”œâ”€> Query Resolution: 2-3 seconds (LLM)
â”œâ”€> Data Extraction: 0.1-0.5 seconds (DuckDB)
â”œâ”€> Validation: <0.1 seconds
â”œâ”€> Synthesis: 3-5 seconds (LLM)
â”‚
Total: 5-9 seconds per query
```

**Optimized System:**
```
User Query
â”‚
â”œâ”€> Cache Check: <0.01 seconds (hit: 80%)
â”‚   â””â”€> Return cached result: ~0.01 seconds
â”‚
â”œâ”€> Cache Miss (20%)
â”‚   â”œâ”€> Query Resolution: 1-2 seconds (caching)
â”‚   â”œâ”€> Data Extraction: 0.2-1 seconds (indexed)
â”‚   â”œâ”€> Validation: <0.1 seconds
â”‚   â””â”€> Synthesis: 2-3 seconds (streaming)
â”‚
Average latency: 0.8-2 seconds per query
```

### Throughput

**Current:** 
- Single instance: ~10 queries/minute
- Limited by LLM API rate limits

**Scaled:**
- Multiple instances + load balancer
- Redis caching
- **Target:** 1000+ queries/minute
- Horizontal scaling as needed

### Database Performance

**DuckDB Benchmarks:**
- 1M row scan: ~100ms
- 10M row scan: ~500ms
- Aggregation (10M rows): ~300ms

**BigQuery Benchmarks:**
- 1B row scan: ~2-5 seconds
- Complex join (100M rows): ~5-10 seconds
- Cached queries: ~1 second

---

## Summary

This architecture provides:

âœ… **Immediate Value**: Working system with sample data
âœ… **Scalability Path**: Clear roadmap to 100GB+
âœ… **Cost Efficiency**: Optimized for performance/cost ratio
âœ… **Maintainability**: Clean separation of concerns
âœ… **Extensibility**: Easy to add new agents/features

The multi-agent design allows for:
- Parallel development of agents
- Easy testing and debugging
- Clear responsibility boundaries
- Flexible workflow modifications

